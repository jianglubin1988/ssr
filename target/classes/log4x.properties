## global settings
#trace.enable=true
#log.enable=true
#sample.ratio=1
trace.probeType=WEB
#trace.entrance=WEB
#用户标识实现类
#logger.context.impl=
## file|kafka|msgFrame
msg.sender=kafka
#msg.sender.trace.topic=cmos-logger-test1
#msg.sender.log.topic=cmos-logger-test1
#msg.sender.batch.size=1

## Kafka settings

kafka.metadata.broker.list=192.168.125.141:9092,192.168.124.164:9092,192.168.124.168:9092
#
#kafka.request.required.acks=-1
#kafka.producer.type=async
#kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
#kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
#kafka.serializer.class=kafka.serializer.StringEncoder
#kafka.compression.type=snappy
#kafka.queue.buffering.max.ms=1000
#kafka.queue.buffering.max.messages=50000
#kafka.batch.num.messages=10000
#kafka.queue.enqueue.timeout.ms=0
#kafka.serializer.encoding=UTF-8
app.log.level=INFO
## logfile settings, if msg.sender=file
#msg.logfile.dir=/workspace/asiainfo/log4x-nj/logtest/
#msg.logfile.maxFileSize=100
#
### max message queue size
#msg.queue.size=100000
### if matched, drop the messages
#msg.content.filter=CheckSVImpl.heartbeat
#
##debug.enable=true
#
### app.server.ip 
#app.server.ip.config=test
#
## 监控日志发送频率
#monitor.log.interval.ms=100
#
#
##Switch of the flush error message, if true the message not produced or not correct finish will be sent to analyser
##flush.error.message.enable=false
##日志文件路径
#failover.filename=../logs/cmos-logger.log
##滚动日志文件模式
#failover.file.pattern=D:/logs/cmos-logger-%{yyyy-MM-dd-HH-mm-ss}.log
##日志滚动频率（仅对TimeRollingFileWriter有效）
#failover.date.pattern=HH_mm
##日志滚动大小阀值，单位字节（仅FixedSizeAppendFileWriter有效）
#failover.log.threshold=102400
##本地日志文件写入实现
#failover.writer=com.cmos.core.logger.file.TimeRollingFileWriter
##日志级别
#bus.log.level=info